def extract_class_methods(java_code):
    function_declarations = []
    try:
        tree = javalang.parse.parse(java_code)
        for path, node in tree:
            if isinstance(node, javalang.tree.MethodDeclaration):
                method_name = node.name
                function_declarations.append(method_name)
    except Exception as e:
       pass
    return function_declarations

def extract_class_relations(java_code):
    tree = javalang.parse.parse(java_code)
    for _, node in tree:
        if isinstance(node, javalang.tree.ClassDeclaration):
            try:
                interfaces = [iface.name for iface in node.implements] if node.implements else []
                super_classes = [sclass.name for sclass in node.extends] if node.extends else []
                return (interfaces,super_classes)
            except Exception as e:
                print(e)
                pass
				
				
for index, row in combined_df.iterrows():
    try:
        if row['type'] == 'CLASS':
            methods = []
            print(f"Getting metadata of: {row['type']} {row['name']}")
            
            try:
                class_methods = extract_class_methods(row['code'])
            except Exception as e:
                print(f"Error extracting methods for {row['name']}: {str(e)}")
                class_methods = []

            file_name = row['file_name']
            
            for class_method in class_methods:
                try:
                    mask = (
                        (combined_df['name'] == class_method) &
                        (combined_df['file_name'] == file_name)
                    )
                    match = combined_df[mask]
                    if not match.empty:
                        first_match = match.iloc[0]
                        methods.append(first_match['id'])
                except Exception as e:
                    print(f"Error processing method {class_method}: {str(e)}")

            try:
                interfaces, super_classes = extract_class_relations(row['code'])
            except Exception as e:
                print(f"Error extracting class relations for {row['name']}: {str(e)}")
                interfaces, super_classes = [], []

            interface_ids = []
            superclass_ids = []

            for interface in interfaces:
                try:
                    mask = (
                        (combined_df['name'] == interface) &
                        (combined_df['type'] == 'INTERFACE')
                    )
                    match = combined_df[mask]
                    if not match.empty:
                        first_match = match.iloc[0]
                        interface_ids.append(first_match['id'])
                except Exception as e:
                    print(f"Error processing interface {interface}: {str(e)}")
            
            for superclass in super_classes:
                try:
                    mask = (
                        (combined_df['name'] == superclass) &
                        (combined_df['type'] == 'CLASS')
                    )
                    match = combined_df[mask]
                    if not match.empty:
                        first_match = match.iloc[0]
                        superclass_ids.append(first_match['id'])
                except Exception as e:
                    print(f"Error processing superclass {superclass}: {str(e)}")

            combined_df.at[index, 'meta_data'] = {
                'methods': methods,
                'interface_implemented': interface_ids,
                'classes_extended': superclass_ids
            }
    except Exception as e:
        print(f"Error processing row {index}: {str(e)}")



for index, row in combined_df.iterrows():
    if row['type'] == 'INTERFACE':
        methods = []
        print('GETTING METADATA OF :', row['type'],row['name'], )
        class_methods = extract_class_methods(row['code'])
        file_name = row['file_name']
        
        for class_method in class_methods:
            mask = (
                (combined_df['name'] == class_method) &
                (combined_df['file_name'] == file_name)
            )
            match = combined_df[mask]
            if not match.empty:
                first_match = match.iloc[0]
                methods.append(first_match['id'])
        combined_df.at[index, 'meta_data'] = {
            'methods': methods
        }
        
		
for _, interface_row in combined_df.iterrows():
    if interface_row['type'] == 'INTERFACE':
        interface_id = interface_row['id']
        print('LINKING METHOD IMPLEMENTATIONS OF INTERFACE:', interface_row['name'], interface_id)
        
        interface_methods = combined_df[combined_df['id'] == interface_id]['meta_data'].values[0]['methods']
        implementing_class = []
        
        for _, class_row in combined_df.iterrows():
            if class_row['type'] == 'CLASS':
                # Check if 'meta_data' exists and is a dictionary
                if isinstance(class_row.get('meta_data'), dict) and 'interface_implemented' in class_row['meta_data']:
                    if interface_id in class_row['meta_data']['interface_implemented']:
                        implementing_class.append(class_row['id'])
        print(implementing_class)
        
        implementing_methods = []
        if len(implementing_class) != 0:
            for class_id in implementing_class:
                print(class_id)
                meta_data = combined_df[combined_df['id'] == class_id]['meta_data'].values[0]
                print(meta_data)
                methods_value = meta_data.get('methods', [])
                print(methods_value)
                implementing_methods.extend(methods_value)
        else:
            print('NO CLASS IMPLEMENTS')
            continue
        print('--',implementing_methods)
        
        for interface_method_id in interface_methods:
            methods_implementing = []
            method_name = combined_df[combined_df['id'] == interface_method_id]['name'].values[0]
            print('FINDING IMPLEMENTATION OF:', method_name)
            print(implementing_methods)
            for implementing_method in implementing_methods:
                impl_method_name = combined_df[combined_df['id'] == implementing_method]['name'].values[0]
                print('-',impl_method_name)
                if impl_method_name == method_name:
                    methods_implementing.append(implementing_method)
            print(methods_implementing)
            
            row = combined_df.loc[combined_df['id'] == interface_method_id]
            if not row.empty:
                index = row.index[0]
                current_meta_data = combined_df.at[index, 'meta_data']
                current_meta_data['methods_implementing'] = methods_implementing
                combined_df.at[index, 'meta_data'] = current_meta_data



def get_total_code(df, start_id):
    """
    Recursively fetches the total code for the given start_id.
    
    Parameters:
    df (pd.DataFrame): The DataFrame containing the mapping information.
    start_id (int): The ID of the starting row.
    
    Returns:
    str: The concatenated code.
    """
    # Base case: if the start_id is not in the DataFrame, return an empty string
    if start_id not in df['id'].values:
        return ""
    
    # Fetch the row with the given start_id
    row = df[df['id'] == start_id].iloc[0]
    
    # If the row type is not 'API', 'CLASS', 'INTERFACE', or 'FUNCTION', return an empty string
    if row['type'] not in ['API', 'CLASS', 'INTERFACE', 'FUNCTION']:
        return ""
    
    total_code = row['code']
    
    # Try to parse the meta_data as a dictionary
    try:
        metadata = ast.literal_eval(row['meta_data']) if row['meta_data'] else {}
        
        # Recursively fetch the code from referenced methods
        for key in ['methods_called', 'methods_implementing']:
            if key in metadata and isinstance(metadata[key], list):
                for method_id in metadata[key]:
                    total_code += get_total_code(df, method_id)
    except (ValueError, SyntaxError):
        # If the meta_data is not a valid dictionary, return the code for the current row
        pass
    
    return total_code




open_ai_prompt = """
you are provided with the code corresponding to a single API. 
Generate the context of the API in the form of JSON with the following format: 
IMPORTANT: "JUST PROVIDE THE JSON AS OUTPUT. DO NOT ADD ON TEXTS"

JSON structure : 
{
    "api_endpoint": (provide the API endpoint, e.g., '/hello'),
    "http_method": (provide the http method of the API, e.g., POST , GET),
    "api_title": (provide a single-line title of the API),
    "short_description" : (provide a two liner short description of the API)
    "long_description": (provide a detailed long detailed paragraph description of the API, including its purpose, functionality, and any relevant details),
    "code_logic": (provide the step-by-step code working explanation of the API code in the 
        form of a list of steps),
    "code_complexity": (categorize as ‘SIMPLE’ , ‘MEDIUM’ or ‘HARD’ based on the complexity of the API, considering data structures, algorithms, lines of code, concurrency, and other relevant factors),
    "tables_linked": [list of table names
    ],
    "language" : (provide the language of the API eg JAVA , PYTHON , etc) 
    "number_of_lines" : (provide the number of lines of code associated with the API)
    "code_quality": (rate the code out of 10 based on overall quality, cleanliness, presence and usefulness of comments, loop enhancements, exception and error handling and any other relevant factors),
    "code_quality_justification" : (give a para to justify the code_quality score given)
    "business_significance": (provide a detailed paragraph explaining the significance of the API with respect to the applications business context),
    “possible_improvements” : (provide a para suggesting improvements that can be done over the API code to increase its efficiency. Keep it focused to code not generalised)
}

Code:
{api_code}"""


useEffect(() => {
    let start = 0;
    const endValue = parseInt(end, 10);
    const increment = endValue / (duration / 10); // Increase the count every 10ms

    const counter = setInterval(() => {
      start += increment;
      if (start >= endValue) {
        clearInterval(counter);
        setCount(endValue);
      } else {
        setCount(Math.floor(start));
      }
    }, 10);

    return () => clearInterval(counter); // Cleanup on unmount
  }, [end, duration]);



import React, { useState, useEffect } from 'react';

const Counter = ({ end, duration = 2000 }) => {
  const [count, setCount] = useState(0);

  useEffect(() => {
    const endValue = Number(end);
    if (isNaN(endValue) || endValue <= 0) return;

    const startTime = performance.now();

    const updateCounter = (currentTime) => {
      const elapsedTime = currentTime - startTime;
      const progress = Math.min(elapsedTime / duration, 1); // Ensure the progress is between 0 and 1

      // Calculate the current count value
      setCount(Math.floor(progress * endValue));

      // If the progress is less than 1, keep animating
      if (progress < 1) {
        requestAnimationFrame(updateCounter);
      }
    };

    requestAnimationFrame(updateCounter);

    return () => {
      setCount(0); // Cleanup if the component unmounts
    };
  }, [end, duration]);

  return <span>{count}</span>;
};

export default Counter;



from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse
import shutil
import zipfile
import os

app = FastAPI()

@app.post("/upload-zip/")
async def upload_zip(
    file: UploadFile = File(...),
    app_name: str = Form(...)
):
    if not file.filename.endswith(".zip"):
        raise HTTPException(status_code=400, detail="Invalid file format. Only .zip files are allowed.")
    
    # Define the base TEMP directory relative to the current working directory
    base_temp_dir = os.path.join(".", "TEMP")
    
    # Ensure the TEMP folder exists; if not, create it
    os.makedirs(base_temp_dir, exist_ok=True)
    
    # Create a specific folder for the zip extraction using the `app_name`
    temp_dir = os.path.join(base_temp_dir, app_name)
    
    # Ensure the folder for the app_name exists; if not, create it
    os.makedirs(temp_dir, exist_ok=True)

    try:
        # Write the uploaded zip file to the temporary directory
        zip_path = os.path.join(temp_dir, file.filename)
        with open(zip_path, "wb") as f:
            shutil.copyfileobj(file.file, f)

        # Extract the zip file contents into the named temporary folder
        with zipfile.ZipFile(zip_path, "r") as zip_ref:
            zip_ref.extractall(temp_dir)

        # Do something with the extracted files here
        # For example, list the files extracted
        extracted_files = os.listdir(temp_dir)

        # Return the list of files as a JSON response along with the app_name
        return JSONResponse(content={
            "app_name": app_name,
            "extracted_files": extracted_files,
            "extracted_to": temp_dir
        })
    
    except zipfile.BadZipFile:
        raise HTTPException(status_code=400, detail="The file is not a valid zip archive.")

    finally:
        # Clean up the temporary directory and its contents after processing
        shutil.rmtree(temp_dir)

# To run the FastAPI app, use: uvicorn main:app --reload


